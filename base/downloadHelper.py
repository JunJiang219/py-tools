import os
import re
import uuid
import requests
import tempfile
import shutil
import threading
from concurrent.futures import ThreadPoolExecutor
from urllib.parse import urlparse, unquote
from datetime import datetime

import time  # ä¿®å¤ RateLimiter ä¸­çš„ time.time()
from requests.adapters import HTTPAdapter  # ä¿®å¤ HTTPAdapter
from urllib3.util.retry import Retry  # ä¿®å¤ Retry

# å…¨å±€æ·»åŠ ä»¤ç‰Œæ¡¶é™æµå™¨
class RateLimiter:
    def __init__(self, rate):
        self.rate = rate  # æ¯ç§’è¯·æ±‚æ•°
        self.tokens = 0
        self.last = time.time()
    
    def acquire(self):
        now = time.time()
        elapsed = now - self.last
        self.tokens = min(self.rate, self.tokens + elapsed * self.rate)
        self.last = now
        
        if self.tokens >= 1:
            self.tokens -= 1
            return True
        return False
    
def download_batch(urls, save_dir, max_workers=8, target_subdir="downloaded", del_dir_existed=False):
    """
    æ‰¹é‡ä¸‹è½½å›¾ç‰‡å¹¶é‡å‘½åä¸´æ—¶ç›®å½•
    
    :param urls: å›¾ç‰‡URLåˆ—è¡¨
    :param save_dir: æœ€ç»ˆä¿å­˜ç›®å½•
    :param max_workers: æœ€å¤§çº¿ç¨‹æ•°
    :param target_subdir: é‡å‘½ååçš„å­ç›®å½•å
    :param del_dir_existed: æ˜¯å¦åˆ é™¤å·²å­˜åœ¨çš„ç›®æ ‡ç›®å½•ï¼ˆé»˜è®¤ä¸ºFalseï¼‰downloaded
    """
    os.makedirs(save_dir, exist_ok=True)  # ç¡®ä¿ç›®å½•å­˜åœ¨
    # æ£€æŸ¥ç›®æ ‡ç›®å½•æ˜¯å¦å­˜åœ¨
    save_dir_existed = True # os.path.exists(save_dir)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    limiter = RateLimiter(rate=15)  # æ ¹æ®æœåŠ¡å™¨æ‰¿å—èƒ½åŠ›è°ƒæ•´
    
    with tempfile.TemporaryDirectory() as tmpdir:
        print(f"ğŸ”„ ä¸´æ—¶ä¸‹è½½ç›®å½•: {tmpdir}")
        lock = threading.Lock()  # åˆ›å»ºçº¿ç¨‹é”
        
        with ThreadPoolExecutor(max_workers) as executor:
            futures = [executor.submit(_download, url, tmpdir, lock, limiter) 
                      for url in urls]
            for future in futures:
                future.result()  # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        
        # ç§»åŠ¨æ•´ä¸ªä¸´æ—¶ç›®å½•åˆ°ç›®æ ‡ä½ç½®
        shutil.move(tmpdir, save_dir)
        print(f"ğŸ“¦ æ–‡ä»¶å·²ç§»åŠ¨åˆ°: {save_dir}")

    # é‡å‘½åå­ç›®å½•ï¼ˆä»…åœ¨ç›®æ ‡ç›®å½•å·²å­˜åœ¨æ—¶æ‰§è¡Œï¼‰
    if save_dir_existed:
        tmpdir_name = os.path.basename(tmpdir)
        source_subdir = os.path.join(save_dir, tmpdir_name)
        target_path = os.path.join(save_dir, target_subdir)
        
        # å®‰å…¨é‡å‘½åï¼ˆé¿å…è¦†ç›–å·²æœ‰ç›®å½•ï¼‰
        if os.path.exists(target_path):
            if del_dir_existed:
                # åˆ é™¤ç°æœ‰ç›®å½•
                shutil.rmtree(target_path)
                print(f"ğŸ—‘ï¸ å·²åˆ é™¤ç°æœ‰ç›®å½•: {target_path}")
            else:
                # backup_name = f"{target_subdir}_backup_{uuid.uuid4().hex[:8]}"
                backup_name = f"{target_subdir}_backup_{timestamp}"
                os.rename(target_path, os.path.join(save_dir, backup_name))
        
        os.rename(source_subdir, target_path)
        print(f"â™»ï¸ å·²å°†ä¸´æ—¶ç›®å½•é‡å‘½åä¸º: {target_subdir}")

def _download(url, save_dir, lock, limiter):
    """
    ä¸‹è½½å•ä¸ªæ–‡ä»¶å¹¶ä¿ç•™åŸå§‹æ–‡ä»¶å
    
    :param url: æ–‡ä»¶URL
    :param save_dir: ä¿å­˜ç›®å½•
    :param lock: çº¿ç¨‹é”
    :param limiter: ä»¤ç‰Œæ¡¶é™æµå™¨
    """
    if not limiter.acquire():
        time.sleep(0.1)  # ç­‰å¾…ä»¤ç‰Œ

    # åˆ›å»ºå¸¦è¿æ¥æ± çš„session
    session = requests.Session()
    adapter = HTTPAdapter(
        pool_connections=100,    # è¿æ¥æ± æ•°é‡
        pool_maxsize=200,        # æœ€å¤§è¿æ¥æ•°
        max_retries=Retry(       # é‡è¯•ç­–ç•¥
            total=3,             # æ€»é‡è¯•æ¬¡æ•°
            backoff_factor=0.5,  # é‡è¯•ç­‰å¾…æ—¶é—´å› å­
            status_forcelist=[500, 502, 503, 504]
        )
    )
    session.mount('https://', adapter)
    
    try:
        response = session.get(url, timeout=(3.05, 30), stream=True)  # åˆ†è¿æ¥/è¯»å–è¶…æ—¶
        response.raise_for_status()
        
        # æå–åŸå§‹æ–‡ä»¶å
        filename = _extract_filename(url, response.headers)
        temp_file = os.path.join(save_dir, f"temp_{uuid.uuid4().hex}.tmp")
        
        # æµå¼ä¸‹è½½åˆ°ä¸´æ—¶æ–‡ä»¶
        with open(temp_file, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:  # è¿‡æ»¤keep-aliveçš„chunk
                    f.write(chunk)
        
        # ä½¿ç”¨é”ç¡®ä¿çº¿ç¨‹å®‰å…¨çš„é‡å‘½å
        with lock:
            final_path = _get_unique_filename(save_dir, filename)
            os.rename(temp_file, final_path)
        
        print(f"âœ… ä¸‹è½½æˆåŠŸ: {os.path.basename(final_path)}")
        return True
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥ [{url}]: {str(e)}")
        return False

def _extract_filename(url, headers):
    """ä¸‰çº§ç­–ç•¥æå–åŸå§‹æ–‡ä»¶å"""
    # 1. ä»Content-Dispositionå¤´è·å–[1](@ref)
    if 'Content-Disposition' in headers:
        match = re.search(r'filename\*?=["\']?(?:UTF-\d[\'"]*)?([^;\'\"]+)["\']?', 
                          headers['Content-Disposition'], re.IGNORECASE)
        if match:
            return unquote(match.group(1).strip())
    
    # 2. ä»URLè·¯å¾„æå–[3](@ref)
    parsed = urlparse(url)
    if parsed.path:
        filename = unquote(parsed.path.rsplit('/', 1)[-1])
        if '.' in filename and len(filename) > 4:  # åŸºæœ¬éªŒè¯
            return filename
    
    # 3. ä½¿ç”¨å†…å®¹ç±»å‹ç”Ÿæˆ[4](@ref)
    content_type = headers.get('Content-Type', '').partition(';')[0].strip()
    ext_map = {
        'image/jpeg': '.jpg',
        'image/png': '.png',
        'image/gif': '.gif',
        'image/webp': '.webp',
        'application/pdf': '.pdf'
    }
    ext = ext_map.get(content_type, '.bin')
    return f"file_{uuid.uuid4().hex}{ext}"

def _get_unique_filename(save_dir, filename):
    """ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶åï¼ˆé¿å…å†²çªï¼‰[6](@ref)"""
    base, ext = os.path.splitext(filename)
    counter = 1
    new_name = filename
    
    while os.path.exists(os.path.join(save_dir, new_name)):
        new_name = f"{base}_{counter}{ext}"
        counter += 1
    
    return os.path.join(save_dir, new_name)

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    image_urls = [
        "https://img.dyn123.com/images/slot-images/PS/25cookieshitthebonus.png",        # ps_25_cookies
        "https://img.dyn123.com/images/slot-images/PS/wolflandholdandwin.png",          # wolf_land
        "https://img.dyn123.com/images/slot-images/PS/sherwoodcoinsholdandwin.png",     # sherwood_coins
        "https://img.dyn123.com/images/slot-images/PS/merrygiftmasholdandwin.png",      # merry_giftmas
        "https://img.dyn123.com/images/slot-images/PS/mammothpeakholdandwin.png",       # mammoth_peak
    ]
    download_batch(
        urls=image_urls,
        save_dir="test/images",
        max_workers=8,
        target_subdir="downloaded",
        del_dir_existed=True
    )