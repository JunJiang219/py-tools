import os
import re
import uuid
import requests
import tempfile
import shutil
import threading
from concurrent.futures import ThreadPoolExecutor
from urllib.parse import urlparse, unquote

def download_batch(urls, save_dir, max_workers=8, target_subdir="downloaded_images"):
    """
    æ‰¹é‡ä¸‹è½½å›¾ç‰‡å¹¶é‡å‘½åä¸´æ—¶ç›®å½•
    
    :param urls: å›¾ç‰‡URLåˆ—è¡¨
    :param save_dir: æœ€ç»ˆä¿å­˜ç›®å½•
    :param max_workers: æœ€å¤§çº¿ç¨‹æ•°
    :param target_subdir: é‡å‘½ååçš„å­ç›®å½•å
    """
    os.makedirs(save_dir, exist_ok=True)  # ç¡®ä¿ç›®å½•å­˜åœ¨
    # æ£€æŸ¥ç›®æ ‡ç›®å½•æ˜¯å¦å­˜åœ¨
    save_dir_existed = True # os.path.exists(save_dir)
    
    with tempfile.TemporaryDirectory() as tmpdir:
        print(f"ğŸ”„ ä¸´æ—¶ä¸‹è½½ç›®å½•: {tmpdir}")
        lock = threading.Lock()  # åˆ›å»ºçº¿ç¨‹é”
        
        with ThreadPoolExecutor(max_workers) as executor:
            futures = [executor.submit(_download, url, tmpdir, lock) 
                      for url in urls]
            for future in futures:
                future.result()  # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        
        # ç§»åŠ¨æ•´ä¸ªä¸´æ—¶ç›®å½•åˆ°ç›®æ ‡ä½ç½®
        shutil.move(tmpdir, save_dir)
        print(f"ğŸ“¦ æ–‡ä»¶å·²ç§»åŠ¨åˆ°: {save_dir}")

    # é‡å‘½åå­ç›®å½•ï¼ˆä»…åœ¨ç›®æ ‡ç›®å½•å·²å­˜åœ¨æ—¶æ‰§è¡Œï¼‰
    if save_dir_existed:
        tmpdir_name = os.path.basename(tmpdir)
        source_subdir = os.path.join(save_dir, tmpdir_name)
        target_path = os.path.join(save_dir, target_subdir)
        
        # å®‰å…¨é‡å‘½åï¼ˆé¿å…è¦†ç›–å·²æœ‰ç›®å½•ï¼‰
        if os.path.exists(target_path):
            backup_name = f"{target_subdir}_backup_{uuid.uuid4().hex[:8]}"
            os.rename(target_path, os.path.join(save_dir, backup_name))
        
        os.rename(source_subdir, target_path)
        print(f"â™»ï¸ å·²å°†ä¸´æ—¶ç›®å½•é‡å‘½åä¸º: {target_subdir}")

def _download(url, save_dir, lock):
    """
    ä¸‹è½½å•ä¸ªæ–‡ä»¶å¹¶ä¿ç•™åŸå§‹æ–‡ä»¶å
    
    :param url: æ–‡ä»¶URL
    :param save_dir: ä¿å­˜ç›®å½•
    :param lock: çº¿ç¨‹é”
    """
    try:
        response = requests.get(url, timeout=15, stream=True)
        response.raise_for_status()
        
        # æå–åŸå§‹æ–‡ä»¶å
        filename = _extract_filename(url, response.headers)
        temp_file = os.path.join(save_dir, f"temp_{uuid.uuid4().hex}.tmp")
        
        # æµå¼ä¸‹è½½åˆ°ä¸´æ—¶æ–‡ä»¶
        with open(temp_file, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:  # è¿‡æ»¤keep-aliveçš„chunk
                    f.write(chunk)
        
        # ä½¿ç”¨é”ç¡®ä¿çº¿ç¨‹å®‰å…¨çš„é‡å‘½å
        with lock:
            final_path = _get_unique_filename(save_dir, filename)
            os.rename(temp_file, final_path)
        
        print(f"âœ… ä¸‹è½½æˆåŠŸ: {os.path.basename(final_path)}")
        return True
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥ [{url}]: {str(e)}")
        return False

def _extract_filename(url, headers):
    """ä¸‰çº§ç­–ç•¥æå–åŸå§‹æ–‡ä»¶å"""
    # 1. ä»Content-Dispositionå¤´è·å–[1](@ref)
    if 'Content-Disposition' in headers:
        match = re.search(r'filename\*?=["\']?(?:UTF-\d[\'"]*)?([^;\'\"]+)["\']?', 
                          headers['Content-Disposition'], re.IGNORECASE)
        if match:
            return unquote(match.group(1).strip())
    
    # 2. ä»URLè·¯å¾„æå–[3](@ref)
    parsed = urlparse(url)
    if parsed.path:
        filename = unquote(parsed.path.rsplit('/', 1)[-1])
        if '.' in filename and len(filename) > 4:  # åŸºæœ¬éªŒè¯
            return filename
    
    # 3. ä½¿ç”¨å†…å®¹ç±»å‹ç”Ÿæˆ[4](@ref)
    content_type = headers.get('Content-Type', '').partition(';')[0].strip()
    ext_map = {
        'image/jpeg': '.jpg',
        'image/png': '.png',
        'image/gif': '.gif',
        'image/webp': '.webp',
        'application/pdf': '.pdf'
    }
    ext = ext_map.get(content_type, '.bin')
    return f"file_{uuid.uuid4().hex}{ext}"

def _get_unique_filename(save_dir, filename):
    """ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶åï¼ˆé¿å…å†²çªï¼‰[6](@ref)"""
    base, ext = os.path.splitext(filename)
    counter = 1
    new_name = filename
    
    while os.path.exists(os.path.join(save_dir, new_name)):
        new_name = f"{base}_{counter}{ext}"
        counter += 1
    
    return os.path.join(save_dir, new_name)

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    image_urls = [
        "https://example.com/images/sunset.jpg",
        "https://example.com/gallery/portrait.png"
    ]
    download_batch(
        urls=image_urls,
        save_dir="test/images",
        max_workers=4,
        target_subdir="downloaded_images"
    )